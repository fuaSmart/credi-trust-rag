{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e675df50",
   "metadata": {},
   "source": [
    "Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f8cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned data from: ../data/processed/filtered_complaints.csv\n",
      "Cleaned dataset loaded successfully!\n",
      "Shape of the cleaned dataset: (82164, 3)\n",
      "\n",
      "First 5 rows of the cleaned dataset (showing relevant columns):\n",
      "   Complaint ID      Product  \\\n",
      "0      14069121  Credit card   \n",
      "1      14047085  Credit card   \n",
      "2      14040217  Credit card   \n",
      "3      13968411  Credit card   \n",
      "4      13965746  Credit card   \n",
      "\n",
      "                Consumer complaint narrative_cleaned  \n",
      "0  a xxxx xxxx card was opened under my name by a...  \n",
      "1  dear cfpb i have a secured credit card with ci...  \n",
      "2  i have a citi rewards cards the credit balance...  \n",
      "3  bi am writing to dispute the following charges...  \n",
      "4  although the account had been deemed closed i ...  \n",
      "\n",
      "Processing 82164 narratives for chunking...\n",
      "Total number of chunks created: 136540\n",
      "\n",
      "Sample of a chunk:\n",
      "page_content='a xxxx xxxx card was opened under my name by a fraudster i received a notice from xxxx that an account was just opened under my name i reached out to xxxx xxxx to state that this activity was unauthorized and not me xxxx xxxx confirmed this was fraudulent and immediately closed the card however they have failed to remove this from the three credit agencies and this fraud is now impacting my credit score based on a hard credit pull done by xxxx xxxx that was done by a fraudster' metadata={'complaint_id': 14069121, 'product': 'Credit card', 'original_index': 0, 'start_index': 0}\n",
      "Content length of first chunk: 91 words\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "cleaned_data_file_path = '../data/processed/filtered_complaints.csv'\n",
    "\n",
    "print(f\"Loading cleaned data from: {cleaned_data_file_path}\")\n",
    "\n",
    "try:\n",
    "    df_cleaned = pd.read_csv(cleaned_data_file_path)\n",
    "    print(\"Cleaned dataset loaded successfully!\")\n",
    "    print(f\"Shape of the cleaned dataset: {df_cleaned.shape}\")\n",
    "    print(\"\\nFirst 5 rows of the cleaned dataset (showing relevant columns):\")\n",
    "\n",
    "    print(df_cleaned[['Complaint ID', 'Product', 'Consumer complaint narrative_cleaned']].head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The cleaned data file was not found at {cleaned_data_file_path}.\")\n",
    "    print(\"Please ensure 'filtered_complaints.csv' is in the data/processed/ directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading the data: {e}\")\n",
    "\n",
    "# --- Text Chunking Strategy ---\n",
    "chunk_size = 1000 \n",
    "chunk_overlap = 100 \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len, \n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "documents_to_chunk = []\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    narrative = str(row['Consumer complaint narrative_cleaned'])\n",
    "    \n",
    "    if narrative.strip():\n",
    "        documents_to_chunk.append({\n",
    "            'page_content': narrative,\n",
    "            'metadata': {\n",
    "                'complaint_id': row['Complaint ID'],\n",
    "                'product': row['Product'],\n",
    "                'original_index': index \n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessing {len(documents_to_chunk)} narratives for chunking...\")\n",
    "\n",
    "# chunking\n",
    "all_chunks = []\n",
    "for doc in documents_to_chunk:\n",
    "    chunks_for_doc = text_splitter.create_documents([doc['page_content']], metadatas=[doc['metadata']])\n",
    "    all_chunks.extend(chunks_for_doc)\n",
    "\n",
    "print(f\"Total number of chunks created: {len(all_chunks)}\")\n",
    "print(\"\\nSample of a chunk:\")\n",
    "if all_chunks:\n",
    "    print(all_chunks[0])\n",
    "    print(f\"Content length of first chunk: {len(all_chunks[0].page_content.split())} words\")\n",
    "\n",
    "# for i in range(5):\n",
    "#     if i < len(all_chunks):\n",
    "#         print(f\"\\nChunk {i+1} content:\\n{all_chunks[i].page_content[:200]}...\") \n",
    "#         print(f\"Chunk {i+1} metadata: {all_chunks[i].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9830f4b",
   "metadata": {},
   "source": [
    " Embedding Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01186c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing embedding model: sentence-transformers/all-MiniLM-L6-v2 (using cpu)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14eb0c16bae467db5017100e2e1e4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e762ef3033f458fb2d52476fdd14485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f19101efdb94e25a0ea1dbb8f114f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aced8644785245d289a9e17a3d0f48f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b338568ce2b4750a0f493f4f04d3a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fua/Documents/credi-trust-rag/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdba95cd3974b2581ef2683adff1a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fua/Documents/credi-trust-rag/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba93c8e505c4cbb8337c855982b43fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11374c821a7547068836f61c745e3252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea858cf7de5e430e9fdc15daf53bb863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc78658d11a543a3a0d96fae2dc67492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f729fc3f786454c8a5ffd7c83164357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d384c0caab45e09ea3f7fa2ee5ecc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model initialized successfully!\n",
      "Sample embedding generated (first 5 values): [-0.028859706595540047, 0.06519326567649841, 0.06909038126468658, 0.027181899175047874, 0.04393984377384186]...\n",
      "Sample embedding dimension: 384\n",
      "\n",
      "Creating FAISS vector store with embeddings...\n",
      "FAISS vector store created successfully!\n",
      "Number of vectors in FAISS index: 136540\n",
      "Vector store saved locally to: ../vector_store\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings \n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'} \n",
    "encode_kwargs = {'normalize_embeddings': False} \n",
    "\n",
    "print(f\"\\nInitializing embedding model: {model_name} (using {model_kwargs['device']})...\")\n",
    "\n",
    "try:\n",
    "    # Initialize the HuggingFaceEmbeddings object\n",
    "    embeddings_model = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    print(\"Embedding model initialized successfully!\")\n",
    "\n",
    "    sample_text = \"This is a test sentence for embedding.\"\n",
    "    sample_embedding = embeddings_model.embed_query(sample_text)\n",
    "    print(f\"Sample embedding generated (first 5 values): {sample_embedding[:5]}...\")\n",
    "    print(f\"Sample embedding dimension: {len(sample_embedding)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing or testing embedding model: {e}\")\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Creating the FAISS vector store\n",
    "print(\"\\nCreating FAISS vector store with embeddings...\")\n",
    "\n",
    "try:\n",
    "    vector_store = FAISS.from_documents(all_chunks, embeddings_model)\n",
    "    print(\"FAISS vector store created successfully!\")\n",
    "    print(f\"Number of vectors in FAISS index: {vector_store.index.ntotal}\")\n",
    "\n",
    "    # --- Persist the Vector Store ---\n",
    "    vector_store_path = \"../vector_store\"\n",
    "    os.makedirs(vector_store_path, exist_ok=True)\n",
    "\n",
    "    vector_store.save_local(vector_store_path)\n",
    "    print(f\"Vector store saved locally to: {vector_store_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating or saving FAISS vector store: {e}\")\n",
    "    print(\"Please ensure 'faiss-cpu' is installed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
