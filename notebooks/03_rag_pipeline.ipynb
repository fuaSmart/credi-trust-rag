{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3133744",
   "metadata": {},
   "source": [
    "# Re-initializing embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d484340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing embedding model: sentence-transformers/all-MiniLM-L6-v2 (using cpu)...\n",
      "Embedding model re-initialized successfully!\n",
      "\n",
      "Loading FAISS vector store from: ../vector_store...\n",
      "FAISS vector store loaded successfully!\n",
      "Number of vectors in loaded FAISS index: 136540\n",
      "\n",
      "Setting up retriever to fetch top 5 chunks...\n",
      "Retriever set up successfully!\n",
      "\n",
      "Testing retriever with sample query: 'problems with unauthorized credit card charges'\n",
      "Retrieved 5 documents.\n",
      "\n",
      "Sample of retrieved document content and metadata:\n",
      "--- Document 1 ---\n",
      "Content (first 200 chars): unauthorized charges on credit account statement shows xxxx xxxx xxxx xxxx xxxx xxxx...\n",
      "Metadata: {'complaint_id': 12508103, 'product': 'Credit card', 'original_index': 7724, 'start_index': 0}\n",
      "--- Document 2 ---\n",
      "Content (first 200 chars): credit card xxxx xxxx was activated by someone else with unauthorized charges along with open old disputes...\n",
      "Metadata: {'complaint_id': 8687697, 'product': 'Credit card', 'original_index': 73169, 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store_path = \"../vector_store\" \n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'} \n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "print(f\"Re-initializing embedding model: {model_name} (using {model_kwargs['device']})...\")\n",
    "\n",
    "try:\n",
    "    embeddings_model = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    print(\"Embedding model re-initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error re-initializing embedding model: {e}\")\n",
    "    print(\"Please ensure 'sentence-transformers' and 'transformers' are installed and working.\")\n",
    "\n",
    "\n",
    "# Load the FAISS vector store\n",
    "print(f\"\\nLoading FAISS vector store from: {vector_store_path}...\")\n",
    "\n",
    "try:\n",
    "    vector_store = FAISS.load_local(vector_store_path, embeddings_model, allow_dangerous_deserialization=True)\n",
    "    print(\"FAISS vector store loaded successfully!\")\n",
    "    print(f\"Number of vectors in loaded FAISS index: {vector_store.index.ntotal}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading FAISS vector store: {e}\")\n",
    "    print(\"Please ensure the 'vector_store' directory exists and contains FAISS index files.\")\n",
    "    print(\"Also ensure the embedding model is correctly re-initialized.\")\n",
    "\n",
    "\n",
    "top_k_chunks = 5 \n",
    "\n",
    "print(f\"\\nSetting up retriever to fetch top {top_k_chunks} chunks...\")\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": top_k_chunks})\n",
    "print(\"Retriever set up successfully!\")\n",
    "\n",
    "sample_query = \"problems with unauthorized credit card charges\"\n",
    "print(f\"\\nTesting retriever with sample query: '{sample_query}'\")\n",
    "\n",
    "try:\n",
    "    retrieved_docs = retriever.invoke(sample_query) \n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents.\")\n",
    "    print(\"\\nSample of retrieved document content and metadata:\")\n",
    "    for i, doc in enumerate(retrieved_docs[:2]): \n",
    "        print(f\"--- Document {i+1} ---\")\n",
    "        print(f\"Content (first 200 chars): {doc.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing retriever: {e}\")\n",
    "    print(\"Ensure the embedding model and vector store are correctly loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097cadf",
   "metadata": {},
   "source": [
    "# Integrate the Large Language Model (LLM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf99d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading LLM tokenizer and model: TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM model loaded successfully!\n",
      "LangChain LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoConfig\n",
    "import torch\n",
    "\n",
    "# Define the LLM model name\n",
    "llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "\n",
    "print(f\"\\nLoading LLM tokenizer and model: {llm_model_name}...\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "\n",
    "    # Load model configuration to check for 'max_position_embeddings'\n",
    "    config = AutoConfig.from_pretrained(llm_model_name)\n",
    "    max_model_length = config.max_position_embeddings if hasattr(config, \"max_position_embeddings\") else 4096 \n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_model_name,\n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=\"cpu\", \n",
    "        trust_remote_code=True \n",
    "    )\n",
    "    print(\"LLM model loaded successfully!\")\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512, \n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "\n",
    "        max_length=min(max_model_length, 2048), \n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "    )\n",
    "\n",
    "    # Initialize LangChain's HuggingFacePipeline LLM\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    print(\"LangChain LLM initialized successfully!\")\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Error: 'transformers' library not found. Please install it: `pip install transformers`\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during LLM setup: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d2c02",
   "metadata": {},
   "source": [
    "-- Test the LLM with a simple prompt --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9dab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing LLM with prompt: '<|user|>\n",
      "What is the capital of France?<|end|>\n",
      "<|assistant|>'\n",
      "\n",
      "--- Full LLM Response (for debugging) ---\n",
      "<|user|>\n",
      "What is the capital of France?<|end|>\n",
      "<|assistant|>\n",
      "The capital of France is Paris, located in the Île-de-France region.\n",
      "\n",
      "--- Extracted LLM Response ---\n",
      "The capital of France is Paris, located in the Île-de-France region.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_prompt = \"<|user|>\\nWhat is the capital of France?<|end|>\\n<|assistant|>\"\n",
    "print(f\"\\nTesting LLM with prompt: '{test_prompt}'\")\n",
    "response = llm.invoke(test_prompt) \n",
    "\n",
    "print(\"\\n--- Full LLM Response (for debugging) ---\")\n",
    "print(response) \n",
    "\n",
    "print(\"\\n--- Extracted LLM Response ---\")\n",
    "\n",
    "try:\n",
    "        start_index = response.find(\"<|assistant|>\")\n",
    "        if start_index != -1:\n",
    "            generated_text = response[start_index + len(\"<|assistant|>\"):].strip()\n",
    "            if generated_text:\n",
    "                print(generated_text.split('\\n')[0])\n",
    "            else:\n",
    "                print(\"[No specific answer generated beyond prompt structure]\")\n",
    "        else:\n",
    "            print(\"Could not find '<|assistant|>' in the response. Full raw output:\")\n",
    "            print(response) \n",
    "except Exception as e:\n",
    "        print(f\"Error during response extraction: {e}\")\n",
    "        print(\"Full raw output:\")\n",
    "        print(response)\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Error: 'transformers' library not found. Please install it: `pip install transformers`\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during LLM setup: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b0b07f",
   "metadata": {},
   "source": [
    "# --- Create the RAG Chain (RetrievalQA) ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc57fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG Chain (RetrievalQA) created successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# --- Define Prompt Template ---\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "You are a financial analyst assistant for CrediTrust. Your task is to answer questions about customer complaints.\n",
    "Use the following retrieved complaint excerpts (Context) to formulate your answer.\n",
    "If the Context doesn't contain enough information to answer the question,\n",
    "state that you don't have enough information based on the provided context.\n",
    "Keep the answer concise and to the point.\n",
    "<|end|>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# --- Create the RAG Chain (RetrievalQA) ---\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "print(\"\\nRAG Chain (RetrievalQA) created successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6132d7",
   "metadata": {},
   "source": [
    "# --- Test the RAG Chain ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing RAG Chain with query: 'What are common complaints about credit cards?'\n",
      "\n",
      "--- RAG Chain Result ---\n",
      "Answer: <|system|>\n",
      "You are a financial analyst assistant for CrediTrust. Your task is to answer questions about customer complaints.\n",
      "Use the following retrieved complaint excerpts (Context) to formulate your answer.\n",
      "If the Context doesn't contain enough information to answer the question,\n",
      "state that you don't have enough information based on the provided context.\n",
      "Keep the answer concise and to the point.\n",
      "<|end|>\n",
      "<|user|>\n",
      "Context:\n",
      "is not right and i am wanting to do a formal complaint with these credit card practices can you please assist best xxxx xxxx xxxx\n",
      "\n",
      "xxxx xxxx xxxx xxxx hide full complaint what product or service is your complaint about product or service credit card type store credit card\n",
      "\n",
      "understatement i have been informed by xxxx xxxx representatives that they have received numerous complaints from xxxx xxxx business mastercard cardholders regarding the same issue\n",
      "\n",
      "consumers i believe this issue raises concerns about compliance with federal credit card billing standards and the clarity required under the card act thank you for your attention to this matter i trust that your investigation will help bring about the necessary changes to protect consumers like myself please feel free to contact me should you require additional information or documentation sincerely xxxx xxxx xxxx\n",
      "\n",
      "who are unaware of the specific conditions tied to the charge card i request that the consumer financial protection bureau investigate this issue and take appropriate action to protect consumers from such deceptive and predatory practices i look forward to your prompt attention to this matter please feel free to contact me at your phone number or your email address for further information or clarification regarding my experience thank you for your time and consideration sincerely your full name feel free to personalize and adjust any details this letter should clearly communicate your complaint while also calling\n",
      "\n",
      "Question: What are common complaints about credit cards?<|end|>\n",
      "<|assistant|>\n",
      "Common complaints about credit cards include understatement of fees, inconsistent billings, lack of clarity on billing standards, and deceptive and predatory practices. The specific complaints vary depending on the credit card type, store, and individual business.\n",
      "\n",
      "--- Retrieved Sources ---\n",
      "Document 1 (Complaint ID: 9455030, Product: Credit card):\n",
      "Content (first 300 chars): is not right and i am wanting to do a formal complaint with these credit card practices can you please assist best xxxx xxxx xxxx...\n",
      "Document 2 (Complaint ID: 12011405, Product: Credit card):\n",
      "Content (first 300 chars): xxxx xxxx xxxx xxxx hide full complaint what product or service is your complaint about product or service credit card type store credit card...\n",
      "Document 3 (Complaint ID: 1919167, Product: Credit card):\n",
      "Content (first 300 chars): understatement i have been informed by xxxx xxxx representatives that they have received numerous complaints from xxxx xxxx business mastercard cardholders regarding the same issue...\n",
      "Document 4 (Complaint ID: 11343091, Product: Credit card):\n",
      "Content (first 300 chars): consumers i believe this issue raises concerns about compliance with federal credit card billing standards and the clarity required under the card act thank you for your attention to this matter i trust that your investigation will help bring about the necessary changes to protect consumers like mys...\n",
      "Document 5 (Complaint ID: 12103781, Product: Credit card):\n",
      "Content (first 300 chars): who are unaware of the specific conditions tied to the charge card i request that the consumer financial protection bureau investigate this issue and take appropriate action to protect consumers from such deceptive and predatory practices i look forward to your prompt attention to this matter please...\n"
     ]
    }
   ],
   "source": [
    "# --- Test the RAG Chain with a sample query ---\n",
    "sample_rag_query = \"What are common complaints about credit cards?\"\n",
    "\n",
    "print(f\"\\nTesting RAG Chain with query: '{sample_rag_query}'\")\n",
    "\n",
    "try:\n",
    "    rag_response = qa_chain.invoke({\"query\": sample_rag_query}) # Use .invoke() for newer LangChain versions\n",
    "    print(\"\\n--- RAG Chain Result ---\")\n",
    "    print(f\"Answer: {rag_response['result']}\")\n",
    "\n",
    "    print(\"\\n--- Retrieved Sources ---\")\n",
    "    for i, doc in enumerate(rag_response['source_documents']):\n",
    "        print(f\"Document {i+1} (Complaint ID: {doc.metadata.get('complaint_id', 'N/A')}, Product: {doc.metadata.get('product', 'N/A')}):\")\n",
    "        # Print first 300 characters of content\n",
    "        print(f\"Content (first 300 chars): {doc.page_content[:300]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during RAG chain invocation: {e}\")\n",
    "    print(\"Ensure LLM, retriever, and prompt are correctly set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124484c1",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44525e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_questions = [\n",
    "    \"What are common complaints about credit cards?\",\n",
    "    \"Are there issues related to billing or fees?\",\n",
    "    \"What kind of deceptive practices are mentioned?\",\n",
    "    \"What problems do consumers face with store credit cards?\",\n",
    "]\n",
    "\n",
    "print(\"Evaluation questions defined.\")\n",
    "\n",
    "# Assuming qa_chain and evaluation_questions are defined in previous cells\n",
    "\n",
    "print(\"\\n--- Running Qualitative Evaluation ---\")\n",
    "evaluation_results = []\n",
    "\n",
    "for i, query in enumerate(evaluation_questions):\n",
    "    print(f\"\\n--- Question {i+1}: {query} ---\")\n",
    "    try:\n",
    "        rag_response = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "        generated_answer = rag_response['result']\n",
    "        source_documents = rag_response['source_documents']\n",
    "\n",
    "        print(f\"Generated Answer: {generated_answer}\")\n",
    "        print(\"\\nRetrieved Sources (Top 2 for evaluation):\")\n",
    "        \n",
    "        # Collect sources for the table\n",
    "        sources_for_table = []\n",
    "        for j, doc in enumerate(source_documents[:2]): \n",
    "            source_info = f\"Document {j+1} (ID: {doc.metadata.get('complaint_id', 'N/A')}, Product: {doc.metadata.get('product', 'N/A')})\"\n",
    "            print(f\"- {source_info}\")\n",
    "            print(f\"  Content (first 150 chars): {doc.page_content[:150]}...\")\n",
    "            sources_for_table.append(source_info) \n",
    "\n",
    "        evaluation_results.append({\n",
    "            \"Question\": query,\n",
    "            \"Generated Answer\": generated_answer,\n",
    "            \"Retrieved Sources\": \"; \".join(sources_for_table), \n",
    "            \"Quality Score\": \"\", \n",
    "            \"Comments/Analysis\": \"\" \n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for query '{query}': {e}\")\n",
    "        evaluation_results.append({\n",
    "            \"Question\": query,\n",
    "            \"Generated Answer\": f\"Error: {e}\",\n",
    "            \"Retrieved Sources\": \"N/A\",\n",
    "            \"Quality Score\": \"0\",\n",
    "            \"Comments/Analysis\": \"System error during generation.\"\n",
    "        })\n",
    "\n",
    "print(\"\\n--- Qualitative Evaluation Run Complete ---\")\n",
    "print(\"\\nBelow is the raw data collected for your evaluation table:\")\n",
    "import json\n",
    "print(json.dumps(evaluation_results, indent=2)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
